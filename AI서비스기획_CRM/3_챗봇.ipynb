{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_챗봇.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"AqDnl_3DP7-p"},"source":["# from google.colab import auth\n","# auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VbMY7b-0QB0J"},"source":["# 경로 설정\n","chat_dir = '/content/gdrive/My Drive/pytest/data/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QIKDbm3MOtO"},"source":["print(chat_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fxI20erDzJ5d"},"source":["# 1. 형태소 분석"]},{"cell_type":"code","metadata":{"id":"taZMmWPmWBt6"},"source":["# 형태소분석기 관련 설치\n","!apt-get update\n","!apt-get install g++ openjdk-8-jdk\n","\n","!pip install JPype1\n","!pip install rhinoMorph"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHOVrxm-Wd7B"},"source":["# 데이터 읽기 함수 정의\n","def read_data(filename, encoding='cp949'):\n","  with open(filename, 'r', encoding=encoding) as f:\n","    data = [line.split('\\t') for line in f.read().splitlines()]\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xfzZ2QMSWtXs"},"source":["# 형태소분석기 준비\n","import rhinoMorph\n","rn = rhinoMorph.startRhino()         # 형태소분석기 기동\n","\n","data = read_data(chat_dir+'data.txt', encoding='cp949')\n","\n","print('자료 타입:', type(data))\n","print('전체 문장수:', len(data))\n","print('형태소 분석 전 모습:', data[:20])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bp_6HRL4XN4z"},"source":["# data 내용 형태소 분석\n","with open(chat_dir+'data_morphed.txt', 'w', encoding='utf-8') as f:\n","    for data_each in data:\n","        morphed_data_each = rhinoMorph.onlyMorph_list(rn, data_each[0], pos=['NNG', 'NNP', 'NP', 'VV', 'VA', 'VX', 'XR', 'IC', 'MM', 'MAG'])\n","        print(\"morphed_data_each:\", morphed_data_each)\n","        joined_data_each = ' '.join(morphed_data_each)\n","        if joined_data_each:\n","            f.write(joined_data_each + '\\t' + data_each[1] + '\\n')\n","    print('Morphological Analysis Completed.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuV6MIXA25e_"},"source":["print(\"연결된 마지막 문장: \", joined_data_each)\n","print(\"마지막 문장의 라벨: \", data_each[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPjdCMU8z60y"},"source":["# 2. 훈련데이터와 테스트데이터 분리"]},{"cell_type":"code","metadata":{"id":"oLFq8Ey3Xf1p"},"source":["def write_data_list(list, filename, encoding):\n","    \"\"\"리스트 변수를 위한 쓰기 함수\"\"\"\n","    with open(chat_dir+filename, 'w') as f:\n","        for item in list:\n","            f.write('%s\\t%s\\n' % (item[0], item[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_Nw5PAUX9Rv"},"source":["# 훈련데이터와 테스트데이터 분리\n","from sklearn.model_selection import train_test_split\n","\n","data = read_data(chat_dir+'data_morphed.txt', encoding='utf-8')\n","train, test = train_test_split(data, test_size=0.2)\n","\n","write_data_list(list=train, filename='train_data_morphed.txt', encoding='utf-8')\n","write_data_list(list=test, filename='test_data_morphed.txt', encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QeNd8ynzZwkw"},"source":["# 훈련 데이터 읽기\n","data = read_data(chat_dir+'train_data_morphed.txt', encoding='utf-8')\n","print('train length:', len(data))\n","\n","texts = [line[0] for line in data]                      # 훈련데이터 본문\n","labels = [line[1] for line in data]                     # 훈련데이터 레이블 부분"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H2DG_W_laHBg"},"source":["# 테스트 데이터 읽기\n","data_val = read_data(chat_dir+'test_data_morphed.txt', encoding='utf-8')\n","print('test length:', len(data_val))\n","\n","texts_val = [line[0] for line in data_val]          # 테스트 데이터 본문\n","labels_val = [line[1] for line in data_val]         # 테스트 데이터 label 부분"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zyboXFT7MTdh"},"source":["# 3. 데이터 변환"]},{"cell_type":"code","metadata":{"id":"vHKxc4gbaQnA"},"source":["# 문자를 숫자로 변환하는 Tokenizing\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import math\n","\n","max_words = 1000                                  # 데이터셋에서 가장 빈도 높은 n개의 단어만 사용한다\n","maxlen = 20                                       # 각 문장의 길이를 고정시킨다.\n","\n","tokenizer = Tokenizer(num_words=max_words)        # 상위빈도 1,000 개의 단어만을 추려내는 Tokenizer 객체 생성\n","tokenizer.fit_on_texts(texts)                     # 단어 인덱스를 구축한다\n","word_index = tokenizer.word_index                 # 단어 인덱스만 가져온다\n","\n","print('전체에서 %s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n","print('word_index type: ', type(word_index))\n","print('word_index: ',word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcr_EzLBax7t"},"source":["# Tokenizing 결과 확인\n","# 상위 빈도 1,000(max_words)개의 단어만 word_index의 숫자 리스트로 변환\n","# Tokenizer 결과가 여기서 반영된다.\n","data = tokenizer.texts_to_sequences(texts)\n","print(\"data:\", data)\n","\n","len_d = [len(d) for d in data]\n","print(\"최대 문장 길이: \", max(len_d))\n","print(\"최소 문장 길이: \", min(len_d))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRWtAFLZa_Kz"},"source":["# data 패딩\n","# 길이를 고정시킨다. maxlen의 수만큼으로 2D 텐서를 만든다. 20을 넘는 데이터는 잘라내고, 모자라는 데이터는 0으로 채운다\n","data = pad_sequences(data, maxlen=maxlen)\n","\n","data_val = tokenizer.texts_to_sequences(texts_val)\n","data_val = pad_sequences(data_val, maxlen=maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRn_4jfA4Fvg"},"source":["print(data_val[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r25YBLyOKW-J"},"source":["print('data type:', type(data))\n","print('data length:', len(data))\n","print('texts 0:', texts[0])\n","print('data 0:', data[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oroglLVsbFlI"},"source":["# 원-핫 인코딩 함수\n","def to_one_hot(sequences, dimension):\n","    results = np.zeros((len(sequences), dimension))\n","    for i, sequence in enumerate(sequences):\n","        results[i, sequence] = 1.\n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cgi6FwiABuR-"},"source":["data = to_one_hot(data, dimension=max_words) \n","data_val = to_one_hot(data_val, dimension=max_words) \n","\n","print('data type:', type(data))\n","print('data length:', len(data))\n","print('texts 0:', texts[0])\n","print('data 0:', data[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3F-sQFjYnSa"},"source":["# 문자열 label을 숫자형 변수로 치환하는 함수\n","def labelToIint(labels):\n","    for count, label in enumerate(labels):\n","        if label == \"배송비\":\n","            labels[count] = 0\n","        elif label == \"담당자문의\":\n","            labels[count] = 1\n","        elif label == \"제품가격\":\n","            labels[count] = 2\n","        elif label == \"배송문의\":\n","            labels[count] = 3\n","        elif label == \"매장코드\":\n","            labels[count] = 4\n","        elif label == \"샘플문의\":\n","            labels[count] = 5\n","        elif label == \"제품불일치\":\n","            labels[count] = 6\n","        elif label == \"반품문의\":\n","            labels[count] = 7\n","        elif label == \"교환문의\":\n","            labels[count] = 8\n","        elif label == \"청구금액\":\n","            labels[count] = 9\n","    return labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzzUV4cz5GKi"},"source":["# 훈련데이터와 테스트데이터의 label을 숫자로 치환\n","print(\"훈련데이터 label 치환 전:\\n\", labels)\n","labels = labelToIint(labels)\n","print(\"치환 후:\", labels)\n","\n","print(\"\\n테스트데이터 label 치환 전:\\n\", labels_val)\n","labels_val = labelToIint(labels_val)\n","print(\"치환 후:\", labels_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZn8F845bKHV"},"source":["# label을 원-핫 인코딩 한다\n","class_number = 10                                 # 분류할 클래스의 수\n","\n","labels = to_one_hot(labels, dimension=class_number)  \n","print(labels)\n","\n","labels_val = to_one_hot(labels_val, dimension=class_number)\n","print(labels_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1O-Z47JbY_A"},"source":["# Train 데이터와 Test 데이터 준비\n","print('데이터 텐서의 크기:', data.shape)          \n","print('레이블 텐서의 크기:', labels.shape)        \n","\n","x_train = data                         \n","y_train = labels                    \n","x_val = data_val\n","y_val = labels_val"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wXhrnSc1ygx"},"source":["# 4. 모델 구성"]},{"cell_type":"code","metadata":{"id":"2qmuwNi0ZAME"},"source":["epochs = 5                                        # 수행할 에포크의 수\n","batch_size = 32                                   # 한 번에 훈련할 배치 사이즈\n","model_name = 'train_data_morphed.h5'              # 저장될 모델의 이름\n","tokenizer_name = 'train_data_morphed.pickle'      # 저장될 토크나이저의 이름"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yi5ZyYxObdJH"},"source":["# Define Model\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Flatten\n","\n","model = Sequential()        # 모델을 새로 정의\n","\n","model.add(Dense(64, activation='relu', input_shape=(max_words,)))\t              # 첫 번째 은닉층\n","model.add(Dense(units=32, activation='relu'))\n","model.add(Dense(units=class_number, activation='softmax'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qrrbFPibwka"},"source":["# Compile Model\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_-ubiiQb2Bu"},"source":["# Train Model\n","# 32개씩 미니 배치를 만들어 10번의 epoch로 훈련\n","# 훈련 데이터로 훈련하고, 검증 데이터로 검증한다\n","# 반환값의 history는 훈련하는 동안 발생한 모든 정보를 담고 있는 딕셔너리이다\n","history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val), verbose=1)\n","history_dict = history.history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8gG3GQ_Bb7rE"},"source":["# Save Model\n","# 만들어진 모델을 이후에 재사용할 수 있도록 저장한다\n","import pickle\n","\n","model.save(model_name)\n","\n","with open(tokenizer_name, 'wb') as file:            # 훈련데이터에서 사용된 상위빈도 1,000개의 단어로 된 Tokenizer 저장(같은 단어를 추출하게 한다)\n","    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dn8UOAFyM9F2"},"source":["# 5. 모델 성능 확인"]},{"cell_type":"code","metadata":{"id":"GpHxz-fmdDv7"},"source":["# Accuracy & Loss\n","# history 딕셔너리 안에 있는 정확도와 손실값을 가져와 본다\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","print('Validation accuracy of each epoch:', np.round(val_acc, 3))\n","epochs = range(1, len(val_acc) + 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnpJ3pVXdFWq"},"source":["# Plotting Performance\n","import matplotlib.pyplot as plt\n","\n","plt.plot(epochs, acc, 'bo', label='Training Acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n","plt.title('Training and Validation accuracy')\n","plt.legend(loc=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rT-b8gE4dMAb"},"source":["plt.plot(epochs, loss, 'bo', label='Training Loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n","plt.title('Training and Validation loss')\n","plt.legend(loc=2)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKEdCELAdsKl"},"source":["# Load Model\n","from tensorflow.keras.models import load_model\n","\n","loaded_model = load_model(model_name)\n","\n","with open(tokenizer_name, 'rb') as handle:\n","    loaded_tokenizer = pickle.load(handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1s-XjxNUeAl0"},"source":["# 라벨을 답변으로 치환하는 함수\n","def intToLabel(label_int):\n","    labels = ''\n","    if label_int == 0:\n","        labels = '20,000원 이상 주문하시면 배송비가 없습니다'\n","    elif label_int == 1:\n","        labels = '담당자는 홈페이지에 직원의 이름 또는 사번을 넣으시면 자세한 정보를 알 수 있습니다'\n","    elif label_int == 2:\n","        labels = '공급가와 소비자가는 홈페이지 > 직원 ID 로그인 > 물품명 > 가격조회 에서 확인 가능합니다'\n","    elif label_int == 3:\n","        labels = '배송에는 보통 2일이 소요되며, 빠른 배송을 선택하시면 1일 안에 책임배달합니다'\n","    elif label_int == 4:\n","        labels = '매장 코드는 홈페이지 > 매장정보 에서 확인 가능합니다'\n","    elif label_int == 5:\n","        labels = '샘플신청은 홈페이지 > 직원 ID 로그인 > 물품명 > 샘플신청 에서 가능합니다'\n","    elif label_int == 6:\n","        labels = '다른 제품이 배송되어 죄송합니다. 홈페이지 > 물품명 > 환불신청 또는 전화 상담 부탁드립니다'\n","    elif label_int == 7:\n","        labels = '배송받으신 모든 제품은 7일 안에는 반품이 가능합니다. 홈페이지 > 물품명 > 환불신청 에서 반품 가능합니다'\n","    elif label_int == 8:\n","        labels = '배송받으신 모든 제품은 7일 안에는 교환이 가능합니다. 홈페이지 > 물품명 > 교환신청 에서 교환 가능합니다'\n","    elif label_int == 9:\n","        labels = '청구금액은 매달 12일 이후에 홈페이지 > 직원 ID 로그인 > 청구금액에서 확인 가능합니다'\n","    return labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MGix7uheHp7"},"source":["# 1문장 예측\n","user_input = input(\"내용을 입력하세요: \")\n","morphed_input, poses = rhinoMorph.wholeResult_list(rn, user_input, pos=['NNG', 'NNP', 'NP', 'VV', 'VA', 'XR', 'VCN', 'MAG', 'MAJ', 'IC', 'JKV', 'EF', 'SF'])\n","text = [morphed_input]\n","\n","data = loaded_tokenizer.texts_to_sequences(text)\n","data = pad_sequences(data, maxlen=maxlen)\n","x_test = to_one_hot(data, dimension=max_words)\n","\n","predictions = loaded_model.predict(x_test)\n","label_int = np.argmax(predictions)\n","label = intToLabel(label_int)\n","print(label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tc8xTzXoiDJy"},"source":["predictions"],"execution_count":null,"outputs":[]}]}